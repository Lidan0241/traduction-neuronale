{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
        "\n",
        "# Télécharge les ressources nécessaires de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialisation du lemmatiseur pour le français\n",
        "lemmatizer = FrenchLefffLemmatizer()\n",
        "\n",
        "def nltk2wordnet(nltk_tag):\n",
        "    # Convertit les étiquettes POS de NLTK en étiquettes WordNet\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def lemmatize_french_file(input_file_path):\n",
        "    # Modifie le chemin du fichier de sortie pour inclure '_lemme' dans le nom\n",
        "    output_file_path = input_file_path.replace('tok.true.clean.fr', '_lemme.fr')\n",
        "\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            # Tokenisation de la ligne en français\n",
        "            tokens = nltk.word_tokenize(line, language='french')\n",
        "            # Étiquetage grammatical en utilisant les étiquettes de NLTK (non optimales pour le français)\n",
        "            nltk_tags = nltk.pos_tag(tokens)\n",
        "            # Conversion des étiquettes NLTK en étiquettes WordNet\n",
        "            wordnet_tags = [(token[0], nltk2wordnet(token[1])) for token in nltk_tags]\n",
        "            # Lemmatisation en utilisant les étiquettes WordNet adaptées, si disponible\n",
        "            lemmatized_tokens = [lemmatizer.lemmatize(token[0], pos=token[1]) if token[1] else token[0] for token in wordnet_tags]\n",
        "            # Écriture de la ligne lemmatisée\n",
        "            lemmatized_line = ' '.join(lemmatized_tokens) + '\\n'\n",
        "            outfile.write(lemmatized_line)\n",
        "\n",
        "# Traitement du fichier d'entrée passé en argument de ligne de commande\n",
        "if len(sys.argv) > 1:\n",
        "    input_file_path = sys.argv[1]\n",
        "    lemmatize_french_file(input_file_path)\n",
        "else:\n",
        "    # Message d'erreur si aucun chemin de fichier n'est fourni\n",
        "    print(\"Veuillez fournir le chemin du fichier d'entrée en argument.\")"
      ],
      "metadata": {
        "id": "5T3JMb3FqOOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}