{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Téléchargement des données nécessaires pour NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialisation du lemmatiseur\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Définition d'une fonction pour obtenir la classe de mot WordNet à partir d'une balise Treebank\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return nltk.corpus.wordnet.ADJ  # Adjectif\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return nltk.corpus.wordnet.VERB  # Verbe\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return nltk.corpus.wordnet.NOUN  # Nom\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return nltk.corpus.wordnet.ADV  # Adverbe\n",
        "    else:\n",
        "        # Par défaut, on considère que c'est un nom\n",
        "        return nltk.corpus.wordnet.NOUN\n",
        "\n",
        "def lemmatize_english_file(input_file_path):\n",
        "\n",
        "    # Remplacement de l'extension du fichier pour créer le nom du fichier de sortie\n",
        "    output_file_path = input_file_path.replace('.tok.true.clean', '_lemme')\n",
        "\n",
        "    # Ouverture du fichier d'entrée et du fichier de sortie\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "        # Lecture et traitement du fichier ligne par ligne\n",
        "        for line in infile:\n",
        "            # Tokenisation de la ligne\n",
        "            words = word_tokenize(line)\n",
        "            # Marquage des parties du discours\n",
        "            tagged_words = pos_tag(words)\n",
        "            # Lemmatisation des mots\n",
        "            lemmas = [\n",
        "                lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_words\n",
        "            ]\n",
        "            # Réassemblage des mots lemmatizés en une phrase et écriture dans le fichier de sortie, en conservant le caractère de nouvelle ligne\n",
        "            outfile.write(' '.join(lemmas) + '\\n')\n",
        "\n",
        "# Vérification des arguments de ligne de commande\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) > 1:\n",
        "        input_file_path = sys.argv[1]\n",
        "        lemmatize_english_file(input_file_path)\n",
        "    else:\n",
        "        print(\"Veuillez fournir le chemin du fichier d'entrée en argument.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg9fhtgK4XxC",
        "outputId": "8cbcec6c-a9d6-476d-8f4c-d8045e42d3a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}